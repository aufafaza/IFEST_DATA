{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da75648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, argparse, warnings, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MaxAbsScaler, normalize\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cba51d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/g9g9w1t577gc00vyhcrn7zmh0000gn/T/ipykernel_15090/3482424505.py:14: UserWarning: transformers/torch not found, BERT-Embedding base will be skipped.\n",
      "  warnings.warn(\"transformers/torch not found, BERT-Embedding base will be skipped.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# optional libs\n",
    "HAS_LGBM = True\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "HAS_TRANSFORMERS = True\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "except Exception:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    warnings.warn(\"transformers/torch not found, BERT-Embedding base will be skipped.\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17ebbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "ID_COL   = \"ID\"\n",
    "TARGET_B = \"Estimate Bottom\"\n",
    "TARGET_U = \"Estimate Up\"\n",
    "TEXT_COLS = [\n",
    "    \"Current Position\",\"Targeted Position\",\"Candidate Level\",\"Domisili\",\n",
    "    \"Education 1\",\"Education 2\",\"Education 3\",\"Notice Period\",\"Tech Stack\",\"Certification\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08143ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- utilities & domain features -----------------\n",
    "def safe_str(x) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    return re.sub(r\"\\s+\",\" \",str(x)).strip()\n",
    "\n",
    "def parse_notice_days(s) -> float:\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).lower().strip()\n",
    "    if s in {\"asap\",\"join asap\",\"can join asap\",\"immediate\",\"immediately\"}: return 0.0\n",
    "    m = re.search(r\"(\\d+(?:[.,]\\d+)?)\", s)\n",
    "    val = float(m.group(1).replace(\",\", \".\")) if m else np.nan\n",
    "    if \"day\" in s or \"hari\" in s: mult=1.0\n",
    "    elif \"week\" in s or \"minggu\" in s or \"wk\" in s: mult=7.0\n",
    "    elif \"month\" in s or \"bulan\" in s or \"bln\" in s or \"mo\" in s: mult=30.0\n",
    "    else: mult=1.0\n",
    "    return val*mult if not np.isnan(val) else np.nan\n",
    "\n",
    "def candidate_level_ord(s: str) -> int:\n",
    "    t = safe_str(s).lower()\n",
    "    if not t: return -1\n",
    "    if re.search(r\"\\b(intern|trainee|fresher|fresh)\\b\", t): return 0\n",
    "    if re.search(r\"\\b(junior|jr\\.?)\\b\", t): return 1\n",
    "    if re.search(r\"\\b(mid|middle|mid[-\\s]?level)\\b\", t): return 2\n",
    "    if re.search(r\"\\b(senior|sr\\.?)\\b\", t): return 3\n",
    "    if re.search(r\"\\b(lead|principal|staff|manager|head|director)\\b\", t): return 4\n",
    "    return -1\n",
    "\n",
    "def edu_ord_from_text(s: str) -> int:\n",
    "    t = safe_str(s).lower()\n",
    "    if not t: return -1\n",
    "    if re.search(r\"\\b(s3|phd|doctor|doktor)\\b\", t): return 4\n",
    "    if re.search(r\"\\b(s2|master|magister|m\\.?sc|m\\.?eng|m\\.?kom|m\\.?ba)\\b\", t): return 3\n",
    "    if re.search(r\"\\b(s1|sarjana|bachelor|b\\.?sc|b\\.?eng|b\\.?kom)\\b\", t): return 2\n",
    "    if re.search(r\"\\b(d3|diploma)\\b\", t): return 1\n",
    "    if re.search(r\"\\b(sma|smk|high\\s?school)\\b\", t): return 0\n",
    "    return -1\n",
    "\n",
    "def highest_edu_ord(row) -> int:\n",
    "    lvls = [edu_ord_from_text(row.get(c, \"\")) for c in [\"Education 1\",\"Education 2\",\"Education 3\"]]\n",
    "    lvls = [x for x in lvls if x >= 0]\n",
    "    return max(lvls) if lvls else -1\n",
    "\n",
    "DELIM = re.compile(r\"[;,/|&+]+|\\band\\b\", re.I)\n",
    "ALNUM = re.compile(r\"[^\\w#+\\.]+\")\n",
    "def tokenize_skills(s: str) -> List[str]:\n",
    "    s = safe_str(s).lower()\n",
    "    if not s or s in {\"none\",\"-\"}: return []\n",
    "    parts = DELIM.split(s)\n",
    "    toks = []\n",
    "    for p in parts:\n",
    "        p = ALNUM.sub(\" \", p).strip()\n",
    "        if not p: continue\n",
    "        p = re.sub(r\"\\d+(\\.\\d+)?\", \"\", p)\n",
    "        p = re.sub(r\"\\s+\", \" \", p).strip()\n",
    "        if p: toks.append(p)\n",
    "    norm=[]\n",
    "    for t in toks:\n",
    "        norm.append(\"html/css\" if t in {\"html\",\"css\",\"html css\"} else t)\n",
    "    return norm\n",
    "\n",
    "def top_k_skills(series: pd.Series, k=50) -> List[str]:\n",
    "    cnt = Counter()\n",
    "    for s in series.fillna(\"\"):\n",
    "        cnt.update(tokenize_skills(s))\n",
    "    for bad in [\"and\",\"basic\",\"expert\",\"beginner\",\"intermediate\",\"none\"]:\n",
    "        cnt.pop(bad, None)\n",
    "    return [w for w,_ in cnt.most_common(k)]\n",
    "\n",
    "def normalize_city(s: str) -> str:\n",
    "    t = safe_str(s).lower()\n",
    "    if not t: return \"\"\n",
    "    t = t.split(\",\")[0]\n",
    "    t = re.sub(r\"\\b(kota|kab\\.?|provinsi|kecamatan|dki)\\b\", \"\", t)\n",
    "    t = re.sub(r\"[^a-z\\s\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def top_k_cities(series: pd.Series, k=20) -> List[str]:\n",
    "    cnt = Counter(normalize_city(s) for s in series.fillna(\"\"))\n",
    "    cnt.pop(\"\", None)\n",
    "    return [c for c,_ in cnt.most_common(k)]\n",
    "\n",
    "def position_similarity(cur: pd.Series, tgt: pd.Series) -> np.ndarray:\n",
    "    vec = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1, lowercase=True)\n",
    "    A = vec.fit_transform(cur.map(safe_str)); B = vec.transform(tgt.map(safe_str))\n",
    "    A = normalize(A); B = normalize(B)\n",
    "    return (A.multiply(B)).sum(axis=1).A1\n",
    "\n",
    "def build_numeric_features(df: pd.DataFrame, ctx: Dict) -> pd.DataFrame:\n",
    "    n = len(df); num = pd.DataFrame(index=df.index)\n",
    "    num[\"total_work_exp\"] = pd.to_numeric(df.get(\"Total Working Experience\", 0), errors=\"coerce\")\n",
    "    num[\"log1p_total_work_exp\"] = np.log1p(np.clip(num[\"total_work_exp\"], 0, None))\n",
    "    num[\"exp_sq\"] = np.square(num[\"total_work_exp\"])\n",
    "    if \"Expected Benefit Botom\" in df.columns:\n",
    "        num[\"expected_bottom\"] = pd.to_numeric(df[\"Expected Benefit Botom\"], errors=\"coerce\")\n",
    "    if \"Expected Benefit Up\" in df.columns:\n",
    "        num[\"expected_up\"] = pd.to_numeric(df[\"Expected Benefit Up\"], errors=\"coerce\")\n",
    "    num[\"expected_mid\"] = (num.get(\"expected_bottom\",0)+num.get(\"expected_up\",0))/2.0\n",
    "    num[\"expected_width\"] = num.get(\"expected_up\",0)-num.get(\"expected_bottom\",0)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        num[\"expected_rel_width\"] = (num[\"expected_width\"]/num[\"expected_mid\"].replace(0,np.nan)).fillna(0)\n",
    "    if \"Notice Period\" in df.columns:\n",
    "        num[\"notice_days\"] = df[\"Notice Period\"].map(parse_notice_days)\n",
    "\n",
    "    for col in [\"Tech Stack\",\"Certification\",\"Current Position\",\"Targeted Position\"]:\n",
    "        s = df.get(col, pd.Series([\"\"]*n)).map(safe_str)\n",
    "        key = col.lower().replace(\" \",\"_\")\n",
    "        num[f\"{key}_len\"] = s.map(len)\n",
    "        if col in [\"Tech Stack\",\"Certification\"]:\n",
    "            if col==\"Tech Stack\":\n",
    "                num[f\"{key}_items\"] = s.map(lambda x: len(tokenize_skills(x)))\n",
    "            else:\n",
    "                num[f\"{key}_items\"] = s.map(lambda x: 0 if x==\"\" else len([t for t in re.split(r\"[;,/|]+\", x) if t.strip()]))\n",
    "\n",
    "    num[\"candidate_level_ord\"] = df.get(\"Candidate Level\", pd.Series([\"\"]*n)).map(candidate_level_ord)\n",
    "    num[\"edu_ord\"] = df.apply(highest_edu_ord, axis=1)\n",
    "\n",
    "    text_combo = (\n",
    "        df.get(\"Current Position\", pd.Series([\"\"]*n)).map(safe_str) + \" | \" +\n",
    "        df.get(\"Targeted Position\", pd.Series([\"\"]*n)).map(safe_str) + \" | \" +\n",
    "        df.get(\"Candidate Level\", pd.Series([\"\"]*n)).map(safe_str)\n",
    "    ).str.lower()\n",
    "    num[\"is_managerial\"] = text_combo.str.contains(r\"\\b(?:head|lead|manager|director)\\b\", regex=True, na=False).astype(int)\n",
    "\n",
    "    city = df.get(\"Domisili\", pd.Series([\"\"]*n)).map(normalize_city)\n",
    "    if ctx and \"top_cities\" in ctx:\n",
    "        for c in ctx[\"top_cities\"]:\n",
    "            num[f\"city_{c}\"] = (city == c).astype(int)\n",
    "    num[\"is_high_umr_city\"] = city.isin(ctx.get(\"high_umr_set\", set())).astype(int)\n",
    "\n",
    "    num[\"has_targeted_position\"] = df.get(\"Targeted Position\", pd.Series(index=df.index)).notna().astype(int)\n",
    "\n",
    "    if ctx and \"top_skills\" in ctx:\n",
    "        ts = df.get(\"Tech Stack\", pd.Series([\"\"]*n)).map(tokenize_skills)\n",
    "        top_set = ctx[\"top_skills\"]\n",
    "        num[\"has_top50_skill\"] = ts.map(lambda lst: int(any(t in top_set for t in lst)))\n",
    "    else:\n",
    "        num[\"has_top50_skill\"] = 0\n",
    "\n",
    "    num = num.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    for c in num.columns:\n",
    "        if num[c].isna().any(): num[c] = num[c].fillna(num[c].median())\n",
    "    return num\n",
    "\n",
    "def join_text_fields(df: pd.DataFrame, cols: List[str]) -> pd.Series:\n",
    "    pieces = [df.get(c, pd.Series([\"\"]*len(df))).map(safe_str) for c in cols]\n",
    "    out = pieces[0]\n",
    "    for p in pieces[1:]: out = out + \" | \" + p\n",
    "    return out\n",
    "\n",
    "def position_similarity_vec(df: pd.DataFrame) -> np.ndarray:\n",
    "    return position_similarity(\n",
    "        df.get(\"Current Position\", pd.Series([\"\"]*len(df))),\n",
    "        df.get(\"Targeted Position\", pd.Series([\"\"]*len(df)))\n",
    "    )\n",
    "\n",
    "def compute_umr_multiplier(df: pd.DataFrame, ctx: dict) -> np.ndarray:\n",
    "    n = len(df); mult = np.ones(n, dtype=float)\n",
    "    if not ctx or \"high_umr_set\" not in ctx or ctx.get(\"umr_boost_pct\",0) <= 0: return mult\n",
    "    base = float(ctx[\"umr_boost_pct\"]); base = min(0.07, max(0.0, base))\n",
    "    city = df.get(\"Domisili\", pd.Series([\"\"]*n)).map(normalize_city)\n",
    "    mask = city.isin(ctx[\"high_umr_set\"]).values\n",
    "    exp = pd.to_numeric(df.get(\"Total Working Experience\", 0), errors=\"coerce\").fillna(0).clip(0, 30).values\n",
    "    exp_scale = np.minimum(1.0, exp / 5.0)\n",
    "    lvl_txt = df.get(\"Candidate Level\", pd.Series([\"\"]*n)).astype(str).tolist()\n",
    "    lvl = np.array([candidate_level_ord(s) for s in lvl_txt])\n",
    "    lvl_scale_map = {-1:0.6, 0:0.5, 1:0.6, 2:1.0, 3:1.15, 4:1.25}\n",
    "    lvl_scale = np.vectorize(lambda x: lvl_scale_map.get(x, 1.0))(lvl)\n",
    "    boost = base * exp_scale * lvl_scale\n",
    "    mult[mask] = 1.0 + boost[mask]\n",
    "    return mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b537216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- text featurizer (TF-IDF/SVD) -----------------\n",
    "@dataclass\n",
    "class TextFeaturizer:\n",
    "    word: TfidfVectorizer\n",
    "    ch: TfidfVectorizer\n",
    "    svd: Optional[TruncatedSVD] = None\n",
    "\n",
    "def fit_text(train_text: pd.Series, max_w=100_000, max_c=80_000, svd_dim: int = 0) -> Tuple[TextFeaturizer, Tuple]:\n",
    "    tfw = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=2, max_features=max_w, lowercase=True)\n",
    "    tfc = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,6), min_df=2, max_features=max_c, lowercase=True)\n",
    "    Xw = tfw.fit_transform(train_text); Xc = tfc.fit_transform(train_text)\n",
    "    if svd_dim and svd_dim>0:\n",
    "        svd = TruncatedSVD(n_components=svd_dim, random_state=SEED)\n",
    "        Xs = svd.fit_transform(hstack([Xw,Xc]))\n",
    "        return TextFeaturizer(tfw,tfc,svd),(Xw,Xc,Xs)\n",
    "    return TextFeaturizer(tfw,tfc,None),(Xw,Xc,None)\n",
    "\n",
    "def transform_text(tf: TextFeaturizer, text: pd.Series) -> Tuple:\n",
    "    Xw = tf.word.transform(text); Xc = tf.ch.transform(text)\n",
    "    if tf.svd is not None:\n",
    "        Xs = tf.svd.transform(hstack([Xw,Xc])); return Xw,Xc,Xs\n",
    "    return Xw,Xc,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24d826af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CV split -----------------\n",
    "def stratified_kfold_by_y(y_mid: np.ndarray, n_splits=5, seed=SEED):\n",
    "    y_mid = np.asarray(y_mid).reshape(-1)\n",
    "    q = min(10, max(2, len(y_mid)//20 or 2))\n",
    "    try:\n",
    "        cats = pd.qcut(pd.Series(y_mid), q=q, duplicates=\"drop\", labels=False)\n",
    "        bins = cats.fillna(0).astype(int).values\n",
    "    except Exception:\n",
    "        ranks = pd.Series(y_mid).rank(method=\"average\", pct=True)\n",
    "        bins = np.floor(ranks * q).clip(0, q-1).astype(int).values\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    return skf.split(np.arange(len(y_mid)), bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb97895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- results container -----------------\n",
    "@dataclass\n",
    "class Result:\n",
    "    name: str\n",
    "    oof: np.ndarray  # (n,2)\n",
    "    mae: float\n",
    "    details: dict\n",
    "    test_pred: Optional[np.ndarray] = None  # filled later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44656ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- base strategies -----------------\n",
    "def build_all_features(df: pd.DataFrame, ctx: Dict, tf_word_char: Optional[TextFeaturizer]=None,\n",
    "                       svd_dim: int = 0, fit_text_on_df: bool = False):\n",
    "    joined = join_text_fields(df, TEXT_COLS)\n",
    "    if fit_text_on_df: tf,(Xw,Xc,Xs) = fit_text(joined, svd_dim=svd_dim)\n",
    "    else:              tf,(Xw,Xc,Xs) = (tf_word_char, transform_text(tf_word_char, joined))\n",
    "    Xn = build_numeric_features(df, ctx); Xn = Xn.copy(); Xn[\"pos_sim\"] = position_similarity_vec(df)\n",
    "    return tf, Xw, Xc, Xs, Xn\n",
    "\n",
    "def ridge_two_stage_cv(train_df: pd.DataFrame, y_bu: np.ndarray, ctx: Dict, folds=5) -> Result:\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid = (y_b+y_u)/2.0; y_w = (y_u-y_b)\n",
    "    oof_mid = np.zeros(len(train_df)); oof_w = np.zeros(len(train_df))\n",
    "    for tr,va in stratified_kfold_by_y(y_mid, n_splits=folds):\n",
    "        tr_df = train_df.iloc[tr]; va_df = train_df.iloc[va]\n",
    "        tf, Xw_tr, Xc_tr, _, Xn_tr = build_all_features(tr_df, ctx, fit_text_on_df=True)\n",
    "        _,  Xw_va, Xc_va, _, Xn_va = build_all_features(va_df, ctx, tf_word_char=tf)\n",
    "        scaler = MaxAbsScaler()\n",
    "        Xn_tr_s = scaler.fit_transform(Xn_tr.values.astype(float)); Xn_va_s = scaler.transform(Xn_va.values.astype(float))\n",
    "        X_tr = hstack([csr_matrix(Xn_tr_s), Xw_tr, Xc_tr]).tocsr()\n",
    "        X_va = hstack([csr_matrix(Xn_va_s), Xw_va, Xc_va]).tocsr()\n",
    "        alphas=[0.1,0.3,0.5,1,1.5,2,3,5,7.5,10,15,25,50]\n",
    "        def fit_predict(target, idx_tr, idx_va):\n",
    "            r1=RidgeCV(alphas=alphas).fit(X_tr, target[idx_tr]); p1=r1.predict(X_va)\n",
    "            tlog=np.log1p(np.maximum(target,0)); r2=RidgeCV(alphas=alphas).fit(X_tr, tlog[idx_tr])\n",
    "            p2=np.expm1(np.clip(r2.predict(X_va), None, 20))\n",
    "            return p2 if mean_absolute_error(target[idx_va], p2) < mean_absolute_error(target[idx_va], p1) else p1\n",
    "        oof_mid[va] = fit_predict(y_mid, tr, va); oof_w[va] = np.clip(fit_predict(y_w, tr, va), 0, None)\n",
    "    pred_b = np.maximum(0, oof_mid - oof_w/2.0); pred_u = np.maximum(0, oof_mid + oof_w/2.0)\n",
    "    sw = pred_b > pred_u\n",
    "    if sw.any():\n",
    "        tb,tu = pred_b.copy(), pred_u.copy()\n",
    "        pred_b[sw]=np.minimum(tb[sw],tu[sw]); pred_u[sw]=np.maximum(tb[sw],tu[sw])\n",
    "    mb = mean_absolute_error(y_b, pred_b); mu = mean_absolute_error(y_u, pred_u)\n",
    "    return Result(\"Ridge-2Stage(TFIDF+num+domain)\", np.column_stack([pred_b,pred_u]), float((mb+mu)/2),\n",
    "                  {\"mae_bottom\":float(mb),\"mae_up\":float(mu)})\n",
    "\n",
    "def hgb_two_stage_cv(train_df: pd.DataFrame, y_bu: np.ndarray, ctx: Dict, folds=5, svd_dim=500) -> Result:\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid = (y_b+y_u)/2.0; y_w = (y_u-y_b)\n",
    "    oof_mid = np.zeros(len(train_df)); oof_w = np.zeros(len(train_df))\n",
    "    for tr,va in stratified_kfold_by_y(y_mid, n_splits=folds):\n",
    "        tr_df = train_df.iloc[tr]; va_df = train_df.iloc[va]\n",
    "        tf, _, _, Xs_tr, Xn_tr = build_all_features(tr_df, ctx, fit_text_on_df=True, svd_dim=svd_dim)\n",
    "        _,  _, _, Xs_va, Xn_va = build_all_features(va_df, ctx, tf_word_char=tf, svd_dim=svd_dim)\n",
    "        X_tr = np.concatenate([Xs_tr, Xn_tr.values], axis=1); X_va = np.concatenate([Xs_va, Xn_va.values], axis=1)\n",
    "        def fit_predict(target, idx_tr, idx_va):\n",
    "            m1=HistGradientBoostingRegressor(random_state=SEED).fit(X_tr, target[idx_tr]); p1=m1.predict(X_va)\n",
    "            tlog=np.log1p(np.maximum(target,0)); m2=HistGradientBoostingRegressor(random_state=SEED).fit(X_tr, tlog[idx_tr])\n",
    "            p2=np.expm1(np.clip(m2.predict(X_va), None, 20))\n",
    "            return p2 if mean_absolute_error(target[idx_va], p2) < mean_absolute_error(target[idx_va], p1) else p1\n",
    "        oof_mid[va]=fit_predict(y_mid,tr,va); oof_w[va]=np.clip(fit_predict(y_w,tr,va),0,None)\n",
    "    pred_b = np.maximum(0, oof_mid - oof_w/2.0); pred_u = np.maximum(0, oof_mid + oof_w/2.0)\n",
    "    sw = pred_b > pred_u\n",
    "    if sw.any():\n",
    "        tb,tu = pred_b.copy(), pred_u.copy()\n",
    "        pred_b[sw]=np.minimum(tb[sw],tu[sw]); pred_u[sw]=np.maximum(tb[sw],tu[sw])\n",
    "    mb = mean_absolute_error(y_b, pred_b); mu = mean_absolute_error(y_u, pred_u)\n",
    "    return Result(\"HGB-2Stage(SVD+num+domain)\", np.column_stack([pred_b,pred_u]), float((mb+mu)/2),\n",
    "                  {\"mae_bottom\":float(mb),\"mae_up\":float(mu)})\n",
    "\n",
    "def lgbm_two_stage_cv(train_df: pd.DataFrame, y_bu: np.ndarray, ctx: Dict, folds=5, svd_dim=500) -> Result:\n",
    "    assert HAS_LGBM, \"lightgbm not available\"\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid = (y_b+y_u)/2.0; y_w = (y_u-y_b)\n",
    "    oof_mid = np.zeros(len(train_df)); oof_w = np.zeros(len(train_df))\n",
    "    params=dict(objective=\"regression\", learning_rate=0.05, n_estimators=800,\n",
    "                num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                reg_alpha=0.1, reg_lambda=1.0, random_state=SEED, verbose=-1)\n",
    "    for tr,va in stratified_kfold_by_y(y_mid, n_splits=folds):\n",
    "        tr_df = train_df.iloc[tr]; va_df = train_df.iloc[va]\n",
    "        tf, _, _, Xs_tr, Xn_tr = build_all_features(tr_df, ctx, fit_text_on_df=True, svd_dim=svd_dim)\n",
    "        _,  _, _, Xs_va, Xn_va = build_all_features(va_df, ctx, tf_word_char=tf, svd_dim=svd_dim)\n",
    "        X_tr = np.concatenate([Xs_tr, Xn_tr.values], axis=1); X_va = np.concatenate([Xs_va, Xn_va.values], axis=1)\n",
    "        def fit_predict(target, idx_tr, idx_va):\n",
    "            m1=lgb.LGBMRegressor(**params).fit(X_tr, target[idx_tr]); p1=m1.predict(X_va)\n",
    "            tlog=np.log1p(np.maximum(target,0)); m2=lgb.LGBMRegressor(**params).fit(X_tr, tlog[idx_tr]); p2=np.expm1(np.clip(m2.predict(X_va), None, 20))\n",
    "            return p2 if mean_absolute_error(target[idx_va], p2) < mean_absolute_error(target[idx_va], p1) else p1\n",
    "        oof_mid[va]=fit_predict(y_mid,tr,va); oof_w[va]=np.clip(fit_predict(y_w,tr,va),0,None)\n",
    "    pred_b = np.maximum(0, oof_mid - oof_w/2.0); pred_u = np.maximum(0, oof_mid + oof_w/2.0)\n",
    "    sw = pred_b > pred_u\n",
    "    if sw.any():\n",
    "        tb,tu = pred_b.copy(), pred_u.copy()\n",
    "        pred_b[sw]=np.minimum(tb[sw],tu[sw]); pred_u[sw]=np.maximum(tb[sw],tu[sw])\n",
    "    mb = mean_absolute_error(y_b, pred_b); mu = mean_absolute_error(y_u, pred_u)\n",
    "    return Result(\"LGBM-2Stage(SVD+num+domain)\", np.column_stack([pred_b,pred_u]), float((mb+mu)/2),\n",
    "                  {\"mae_bottom\":float(mb),\"mae_up\":float(mu)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2ecd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NEW: BERT-Embedding + Ridge two-stage (no AutoML) ----\n",
    "def bert_mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = (last_hidden_state * mask).sum(1)\n",
    "    counts = mask.sum(1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str, max_len=160, batch=32, device=None):\n",
    "    assert HAS_TRANSFORMERS, \"transformers not installed\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModel.from_pretrained(model_name)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mdl.to(device); mdl.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch):\n",
    "            t = texts[i:i+batch]\n",
    "            enc = tok(list(t), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "            enc = {k:v.to(device) for k,v in enc.items()}\n",
    "            out = mdl(**enc)\n",
    "            pooled = bert_mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "            embs.append(pooled.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def bert_ridge_two_stage_cv(train_df: pd.DataFrame, y_bu: np.ndarray, ctx: Dict,\n",
    "                            folds=5, model_name=\"indobenchmark/indobert-base-p2\",\n",
    "                            max_len=160, batch=32) -> Result:\n",
    "    assert HAS_TRANSFORMERS, \"transformers not available\"\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid=(y_b+y_u)/2.0; y_w=(y_u-y_b)\n",
    "    # precompute embeddings for ALL train once\n",
    "    joined = join_text_fields(train_df, TEXT_COLS).tolist()\n",
    "    E = embed_texts(joined, model_name=model_name, max_len=max_len, batch=batch)\n",
    "    # add numeric features\n",
    "    Xn = build_numeric_features(train_df, ctx).values\n",
    "    E_all = np.concatenate([E, Xn], axis=1)\n",
    "\n",
    "    oof_mid = np.zeros(len(train_df)); oof_w = np.zeros(len(train_df))\n",
    "    alphas=[0.1,0.3,0.5,1,2,3,5,7.5,10,15,25,50]\n",
    "    for tr,va in stratified_kfold_by_y(y_mid, n_splits=folds):\n",
    "        X_tr, X_va = E_all[tr], E_all[va]\n",
    "        def fit_predict(target):\n",
    "            r1 = RidgeCV(alphas=alphas).fit(X_tr, target[tr]); p1 = r1.predict(X_va)\n",
    "            tlog=np.log1p(np.maximum(target,0)); r2=RidgeCV(alphas=alphas).fit(X_tr, tlog[tr]); p2=np.expm1(np.clip(r2.predict(X_va), None, 20))\n",
    "            return p2 if mean_absolute_error(target[va], p2) < mean_absolute_error(target[va], p1) else p1\n",
    "        oof_mid[va] = fit_predict(y_mid); oof_w[va] = np.clip(fit_predict(y_w), 0, None)\n",
    "\n",
    "    pred_b = np.maximum(0, oof_mid - oof_w/2.0); pred_u = np.maximum(0, oof_mid + oof_w/2.0)\n",
    "    sw = pred_b > pred_u\n",
    "    if sw.any():\n",
    "        tb,tu = pred_b.copy(), pred_u.copy()\n",
    "        pred_b[sw]=np.minimum(tb[sw],tu[sw]); pred_u[sw]=np.maximum(tb[sw],tu[sw])\n",
    "    mb = mean_absolute_error(y_b, pred_b); mu = mean_absolute_error(y_u, pred_u)\n",
    "    return Result(\"BERTEmbed-2Stage(Ridge+num)\", np.column_stack([pred_b,pred_u]),\n",
    "                  float((mb+mu)/2), {\"mae_bottom\":float(mb),\"mae_up\":float(mu),\n",
    "                  \"model\":model_name, \"max_len\":int(max_len)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e78bd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- meta-stacking & calibration -----------------\n",
    "def fit_meta_and_calibrate(res_list: List[Result], y: np.ndarray):\n",
    "    # stack per-target\n",
    "    oof_b = np.column_stack([r.oof[:,0] for r in res_list])\n",
    "    oof_u = np.column_stack([r.oof[:,1] for r in res_list])\n",
    "    ridge_grid=[0.001,0.01,0.1,1,3,10,30,100]\n",
    "    meta_b = RidgeCV(alphas=ridge_grid).fit(oof_b, y[:,0])\n",
    "    meta_u = RidgeCV(alphas=ridge_grid).fit(oof_u, y[:,1])\n",
    "    oof_meta = np.column_stack([meta_b.predict(oof_b), meta_u.predict(oof_u)])\n",
    "\n",
    "    cal_b = IsotonicRegression(out_of_bounds=\"clip\").fit(oof_meta[:,0], y[:,0])\n",
    "    cal_u = IsotonicRegression(out_of_bounds=\"clip\").fit(oof_meta[:,1], y[:,1])\n",
    "\n",
    "    mb = mean_absolute_error(y[:,0], cal_b.transform(oof_meta[:,0]))\n",
    "    mu = mean_absolute_error(y[:,1], cal_u.transform(oof_meta[:,1]))\n",
    "    return {\"meta_b\":meta_b, \"meta_u\":meta_u, \"cal_b\":cal_b, \"cal_u\":cal_u,\n",
    "            \"oof_mae\": (mb+mu)/2, \"oof_mb\": mb, \"oof_mu\": mu}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf05c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- full-train & predict each base -----------------\n",
    "def predict_test_ridge(train_df, test_df, y_bu, ctx):\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]\n",
    "    tf, Xw_tr, Xc_tr, _, Xn_tr = build_all_features(train_df, ctx, fit_text_on_df=True)\n",
    "    _,  Xw_te, Xc_te, _, Xn_te = build_all_features(test_df,  ctx, tf_word_char=tf)\n",
    "    scaler=MaxAbsScaler(); Xn_s=scaler.fit_transform(Xn_tr.values.astype(float)); Xn_ts=scaler.transform(Xn_te.values.astype(float))\n",
    "    X=hstack([csr_matrix(Xn_s),Xw_tr,Xc_tr]).tocsr(); Xte=hstack([csr_matrix(Xn_ts),Xw_te,Xc_te]).tocsr()\n",
    "    alphas=[0.1,0.3,0.5,1,2,3,5,7.5,10,15,25,50]\n",
    "    y_mid=(y_b+y_u)/2; y_w=(y_u-y_b)\n",
    "    r1=RidgeCV(alphas=alphas).fit(X,y_mid); p1=r1.predict(Xte)\n",
    "    r2=RidgeCV(alphas=alphas).fit(X,np.log1p(np.maximum(y_mid,0))); p2=np.expm1(np.clip(r2.predict(Xte), None, 20))\n",
    "    mid=np.minimum(p1,p2)\n",
    "    r3=RidgeCV(alphas=alphas).fit(X,y_w); q1=r3.predict(Xte)\n",
    "    r4=RidgeCV(alphas=alphas).fit(X,np.log1p(np.maximum(y_w,0))); q2=np.expm1(np.clip(r4.predict(Xte), None, 20))\n",
    "    width=np.maximum(0,np.minimum(q1,q2)); b=np.maximum(0, mid-width/2); u=np.maximum(0, mid+width/2)\n",
    "    return np.column_stack([b,u])\n",
    "\n",
    "def predict_test_hgb(train_df, test_df, y_bu, ctx, svd_dim=500):\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid=(y_b+y_u)/2; y_w=(y_u-y_b)\n",
    "    tf, _, _, Xs_tr, Xn_tr = build_all_features(train_df, ctx, fit_text_on_df=True, svd_dim=svd_dim)\n",
    "    _,  _, _, Xs_te, Xn_te = build_all_features(test_df,  ctx, tf_word_char=tf, svd_dim=svd_dim)\n",
    "    X=np.concatenate([Xs_tr,Xn_tr.values],axis=1); Xte=np.concatenate([Xs_te,Xn_te.values],axis=1)\n",
    "    m1=HistGradientBoostingRegressor(random_state=SEED).fit(X,y_mid); p1=m1.predict(Xte)\n",
    "    m2=HistGradientBoostingRegressor(random_state=SEED).fit(X,np.log1p(np.maximum(y_mid,0))); p2=np.expm1(np.clip(m2.predict(Xte), None, 20))\n",
    "    mid=np.minimum(p1,p2); m3=HistGradientBoostingRegressor(random_state=SEED).fit(X,y_w); q1=m3.predict(Xte)\n",
    "    m4=HistGradientBoostingRegressor(random_state=SEED).fit(X,np.log1p(np.maximum(y_w,0))); q2=np.expm1(np.clip(m4.predict(Xte), None, 20))\n",
    "    width=np.maximum(0,np.minimum(q1,q2)); b=np.maximum(0, mid-width/2); u=np.maximum(0, mid+width/2)\n",
    "    return np.column_stack([b,u])\n",
    "\n",
    "def predict_test_lgbm(train_df, test_df, y_bu, ctx, svd_dim=500):\n",
    "    assert HAS_LGBM\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid=(y_b+y_u)/2; y_w=(y_u-y_b)\n",
    "    tf, _, _, Xs_tr, Xn_tr = build_all_features(train_df, ctx, fit_text_on_df=True, svd_dim=svd_dim)\n",
    "    _,  _, _, Xs_te, Xn_te = build_all_features(test_df,  ctx, tf_word_char=tf, svd_dim=svd_dim)\n",
    "    X=np.concatenate([Xs_tr,Xn_tr.values],axis=1); Xte=np.concatenate([Xs_te,Xn_te.values],axis=1)\n",
    "    params=dict(objective=\"regression\", learning_rate=0.05, n_estimators=800, num_leaves=63, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0, random_state=SEED, verbose=-1)\n",
    "    m1=lgb.LGBMRegressor(**params).fit(X,y_mid); p1=m1.predict(Xte)\n",
    "    m2=lgb.LGBMRegressor(**params).fit(X,np.log1p(np.maximum(y_mid,0))); p2=np.expm1(np.clip(m2.predict(Xte), None, 20))\n",
    "    mid=np.minimum(p1,p2); m3=lgb.LGBMRegressor(**params).fit(X,y_w); q1=m3.predict(Xte)\n",
    "    m4=lgb.LGBMRegressor(**params).fit(X,np.log1p(np.maximum(y_w,0))); q2=np.expm1(np.clip(m4.predict(Xte), None, 20))\n",
    "    width=np.maximum(0,np.minimum(q1,q2)); b=np.maximum(0, mid-width/2); u=np.maximum(0, mid+width/2)\n",
    "    return np.column_stack([b,u])\n",
    "\n",
    "def predict_test_bert(train_df, test_df, y_bu, ctx, model_name=\"indobenchmark/indobert-base-p2\", max_len=160, batch=32):\n",
    "    assert HAS_TRANSFORMERS\n",
    "    y_b = y_bu[:,0]; y_u = y_bu[:,1]; y_mid=(y_b+y_u)/2; y_w=(y_u-y_b)\n",
    "    Et = embed_texts(join_text_fields(train_df, TEXT_COLS).tolist(), model_name, max_len, batch)\n",
    "    En = build_numeric_features(train_df, ctx).values\n",
    "    X = np.concatenate([Et, En], axis=1)\n",
    "    Ee = embed_texts(join_text_fields(test_df, TEXT_COLS).tolist(), model_name, max_len, batch)\n",
    "    En_te = build_numeric_features(test_df, ctx).values\n",
    "    Xte = np.concatenate([Ee, En_te], axis=1)\n",
    "    alphas=[0.1,0.3,0.5,1,2,3,5,7.5,10,15,25,50]\n",
    "    r1=RidgeCV(alphas=alphas).fit(X,y_mid); p1=r1.predict(Xte)\n",
    "    r2=RidgeCV(alphas=alphas).fit(X,np.log1p(np.maximum(y_mid,0))); p2=np.expm1(np.clip(r2.predict(Xte), None, 20))\n",
    "    mid=np.minimum(p1,p2); r3=RidgeCV(alphas=alphas).fit(X,y_w); q1=r3.predict(Xte)\n",
    "    r4=RidgeCV(alphas=alphas).fit(X,np.log1p(np.maximum(y_w,0))); q2=np.expm1(np.clip(r4.predict(Xte), None, 20))\n",
    "    width=np.maximum(0,np.minimum(q1,q2)); b=np.maximum(0, mid-width/2); u=np.maximum(0, mid+width/2)\n",
    "    return np.column_stack([b,u])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "487244b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- main -----------------\n",
    "def main(argv=None):\n",
    "    import argparse, json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ap = argparse.ArgumentParser(allow_abbrev=False)\n",
    "\n",
    "    # 1) DEFINISIKAN SEMUA ARGUMEN DULU\n",
    "    ap.add_argument(\"--train\", default=\"train.csv\")\n",
    "    ap.add_argument(\"--test\",  default=\"test.csv\")\n",
    "    ap.add_argument(\"--out\",   default=\"submission.csv\")\n",
    "    ap.add_argument(\"--folds\", type=int, default=5)\n",
    "    ap.add_argument(\"--svd_dim\", type=int, default=500)\n",
    "    ap.add_argument(\"--plot\", default=\"model_mae.png\")\n",
    "    ap.add_argument(\"--top_skills\", type=int, default=50)\n",
    "    ap.add_argument(\"--top_cities\", type=int, default=20)\n",
    "    ap.add_argument(\"--umr_boost_pct\", type=float, default=0.02)\n",
    "    ap.add_argument(\"--umr_cities\", type=str,\n",
    "                    default=\"kota bekasi,kabupaten karawang,kabupaten bekasi,dki jakarta,kota depok\")\n",
    "    # BERT options\n",
    "    ap.add_argument(\"--use_bert\", action=\"store_true\")\n",
    "    ap.add_argument(\"--bert_model\", type=str, default=\"indobenchmark/indobert-base-p2\")\n",
    "    ap.add_argument(\"--bert_maxlen\", type=int, default=160)\n",
    "    ap.add_argument(\"--bert_batch\", type=int, default=32)\n",
    "    ap.add_argument(\"--include_lgbm\", action=\"store_true\")\n",
    "\n",
    "    # 2) BARU DIPARSE\n",
    "    if argv is None:\n",
    "        # di notebook, abaikan argumen aneh dari kernel Jupyter\n",
    "        args, _ = ap.parse_known_args()\n",
    "    else:\n",
    "        args = ap.parse_args(argv)\n",
    "\n",
    "    # ====== kode kamu di bawah ini (tidak diubah) ======\n",
    "    train_df = pd.read_csv(args.train)\n",
    "    test_df  = pd.read_csv(args.test)\n",
    "    y = train_df[[TARGET_B, TARGET_U]].values.astype(float)\n",
    "\n",
    "    # context (skills, cities, UMR)\n",
    "    top_sk = top_k_skills(train_df.get(\"Tech Stack\", pd.Series([], dtype=str)), k=args.top_skills)\n",
    "    top_ct = top_k_cities(train_df.get(\"Domisili\", pd.Series([], dtype=str)), k=args.top_cities)\n",
    "    high_umr_set = set(normalize_city(s.strip()) for s in str(args.umr_cities).split(\",\") if s.strip())\n",
    "    ctx = {\"top_skills\": set(top_sk), \"top_cities\": top_ct,\n",
    "           \"high_umr_set\": high_umr_set, \"umr_boost_pct\": float(args.umr_boost_pct)}\n",
    "\n",
    "    results: List[Result] = []\n",
    "\n",
    "    print(\"CV: HGB two-stage (+domain feats) ...\")\n",
    "    r_h = hgb_two_stage_cv(train_df, y, ctx, folds=args.folds, svd_dim=args.svd_dim)\n",
    "    print(\" ->\", r_h.details, \"avg:\", r_h.mae); results.append(r_h)\n",
    "\n",
    "    print(\"CV: Ridge two-stage (+domain feats) ...\")\n",
    "    r_r = ridge_two_stage_cv(train_df, y, ctx, folds=args.folds)\n",
    "    print(\" ->\", r_r.details, \"avg:\", r_r.mae); results.append(r_r)\n",
    "\n",
    "    if getattr(args, \"include_lgbm\", False) and 'HAS_LGBM' in globals() and HAS_LGBM:\n",
    "        print(\"CV: LGBM two-stage (+domain feats) ...\")\n",
    "        r_l = lgbm_two_stage_cv(train_df, y, ctx, folds=args.folds, svd_dim=args.svd_dim)\n",
    "        print(\" ->\", r_l.details, \"avg:\", r_l.mae); results.append(r_l)\n",
    "\n",
    "    if getattr(args, \"use_bert\", False) and 'HAS_TRANSFORMERS' in globals() and HAS_TRANSFORMERS \\\n",
    "       and \"bert_ridge_two_stage_cv\" in globals():\n",
    "        print(f\"CV: BERT-Embedding two-stage ({args.bert_model}) ...\")\n",
    "        r_b = bert_ridge_two_stage_cv(train_df, y, ctx, folds=args.folds,\n",
    "                                      model_name=args.bert_model, max_len=args.bert_maxlen, batch=args.bert_batch)\n",
    "        print(\" ->\", r_b.details, \"avg:\", r_b.mae); results.append(r_b)\n",
    "\n",
    "    results = sorted(results, key=lambda r: r.mae)\n",
    "\n",
    "    meta = fit_meta_and_calibrate(results, y)\n",
    "    print(f\"[STACK] Calibrated OOF MAE -> avg: {meta['oof_mae']:.0f} (bottom {meta['oof_mb']:.0f} | up {meta['oof_mu']:.0f})\")\n",
    "\n",
    "    names  = [r.name for r in results] + [\"Meta+Calibrated\"]\n",
    "    scores = [r.mae for r in results]  + [meta[\"oof_mae\"]]\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.barh(names, scores); plt.xlabel(\"OOF MAE (smaller is better)\")\n",
    "    plt.title(\"Model Comparison + Meta\"); plt.gca().invert_yaxis(); plt.tight_layout()\n",
    "    plt.savefig(args.plot)\n",
    "    print(f\"Saved plot: {args.plot}\")\n",
    "\n",
    "    preds_test = []\n",
    "    preds_test.append(predict_test_hgb(train_df, test_df, y, ctx, svd_dim=args.svd_dim))\n",
    "    preds_test.append(predict_test_ridge(train_df, test_df, y, ctx))\n",
    "    if getattr(args, \"include_lgbm\", False) and 'HAS_LGBM' in globals() and HAS_LGBM:\n",
    "        preds_test.append(predict_test_lgbm(train_df, test_df, y, ctx, svd_dim=args.svd_dim))\n",
    "    if getattr(args, \"use_bert\", False) and 'HAS_TRANSFORMERS' in globals() and HAS_TRANSFORMERS \\\n",
    "       and \"predict_test_bert\" in globals():\n",
    "        preds_test.append(predict_test_bert(train_df, test_df, y, ctx,\n",
    "                                            model_name=args.bert_model, max_len=args.bert_maxlen, batch=args.bert_batch))\n",
    "\n",
    "    test_stack_b = np.column_stack([p[:,0] for p in preds_test])\n",
    "    test_stack_u = np.column_stack([p[:,1] for p in preds_test])\n",
    "    pred_b = meta[\"meta_b\"].predict(test_stack_b)\n",
    "    pred_u = meta[\"meta_u\"].predict(test_stack_u)\n",
    "\n",
    "    umr_mult = compute_umr_multiplier(test_df, ctx)\n",
    "    pred_b *= umr_mult; pred_u *= umr_mult\n",
    "\n",
    "    pred_b = meta[\"cal_b\"].transform(pred_b)\n",
    "    pred_u = meta[\"cal_u\"].transform(pred_u)\n",
    "    sw = pred_b > pred_u\n",
    "    if sw.any():\n",
    "        tb, tu = pred_b.copy(), pred_u.copy()\n",
    "        pred_b[sw] = np.minimum(tb[sw], tu[sw])\n",
    "        pred_u[sw] = np.maximum(tb[sw], tu[sw])\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        ID_COL: test_df[ID_COL].values,\n",
    "        TARGET_B: np.round(pred_b).astype(int),\n",
    "        TARGET_U: np.round(pred_u).astype(int),\n",
    "    })\n",
    "    sub.to_csv(args.out, index=False)\n",
    "\n",
    "    report = {\n",
    "        \"bases\": [{ \"name\": r.name, \"mae\": float(r.mae), **r.details } for r in results],\n",
    "        \"stack_oof_mae\": float(meta[\"oof_mae\"]),\n",
    "        \"umr\": {\"boost_pct\": float(args.umr_boost_pct), \"cities\": list(high_umr_set)},\n",
    "        \"params\": {\"folds\": args.folds, \"svd_dim\": args.svd_dim,\n",
    "                   \"use_bert\": bool(getattr(args, \"use_bert\", False)),\n",
    "                   \"bert_model\": args.bert_model if getattr(args, \"use_bert\", False) else None}\n",
    "    }\n",
    "    print(json.dumps(report, indent=2))\n",
    "    print(f\"Saved submission: {args.out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45141108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: HGB two-stage (+domain feats) ...\n",
      " -> {'mae_bottom': 1029867.5890121871, 'mae_up': 1489490.4233241233} avg: 1259679.0061681552\n",
      "CV: Ridge two-stage (+domain feats) ...\n",
      " -> {'mae_bottom': 1062618.568444418, 'mae_up': 1590902.098827598} avg: 1326760.333636008\n",
      "[STACK] Calibrated OOF MAE -> avg: 1079313 (bottom 860520 | up 1298106)\n",
      "Saved plot: model_mae.png\n",
      "{\n",
      "  \"bases\": [\n",
      "    {\n",
      "      \"name\": \"HGB-2Stage(SVD+num+domain)\",\n",
      "      \"mae\": 1259679.0061681552,\n",
      "      \"mae_bottom\": 1029867.5890121871,\n",
      "      \"mae_up\": 1489490.4233241233\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ridge-2Stage(TFIDF+num+domain)\",\n",
      "      \"mae\": 1326760.333636008,\n",
      "      \"mae_bottom\": 1062618.568444418,\n",
      "      \"mae_up\": 1590902.098827598\n",
      "    }\n",
      "  ],\n",
      "  \"stack_oof_mae\": 1079313.0185164944,\n",
      "  \"umr\": {\n",
      "    \"boost_pct\": 0.02,\n",
      "    \"cities\": [\n",
      "      \"kabupaten bekasi\",\n",
      "      \"depok\",\n",
      "      \"kabupaten karawang\",\n",
      "      \"bekasi\",\n",
      "      \"jakarta\"\n",
      "    ]\n",
      "  },\n",
      "  \"params\": {\n",
      "    \"folds\": 5,\n",
      "    \"svd_dim\": 500,\n",
      "    \"use_bert\": false,\n",
      "    \"bert_model\": null\n",
      "  }\n",
      "}\n",
      "Saved submission: submission.csv\n"
     ]
    }
   ],
   "source": [
    "main([\n",
    "  \"--train\",\"train.csv\",\n",
    "  \"--test\",\"test.csv\",\n",
    "  \"--out\",\"submission.csv\",\n",
    "  \"--folds\",\"5\",\n",
    "  \"--svd_dim\",\"500\",\n",
    "  \"--top_skills\",\"50\",\n",
    "  \"--top_cities\",\"20\",\n",
    "  \"--umr_boost_pct\",\"0.02\",\n",
    "  \"--umr_cities\",\"kota bekasi,kabupaten karawang,kabupaten bekasi,dki jakarta,kota depok\",\n",
    "  \"--include_lgbm\",           # kalau mau aktifkan LGBM\n",
    "  # \"--use_bert\",             # aktifkan kalau fungsi BERT kamu memang ada\n",
    "  # \"--bert_model\",\"indobenchmark/indobert-base-p2\",\n",
    "  # \"--bert_maxlen\",\"160\",\n",
    "  # \"--bert_batch\",\"32\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b28e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a35028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738eb77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
